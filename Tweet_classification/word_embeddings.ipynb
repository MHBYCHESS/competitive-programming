{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Vaccinate or Not to Vaccinate: Analysing social media sentiment towards vaccines\n",
    "\n",
    "Although it may be many months before we see COVID-19 vaccines available on a global scale, it is important to monitor public sentiment towards vaccinations now and especially in the future when COVID-19 vaccines are offered to the public. The anti-vaccination sentiment could pose a serious threat to the global efforts to get COVID-19 under control in the long term.\n",
    "\n",
    "The objective of this challenge is to develop a machine learning model to assess if a Twitter post related to vaccinations is positive, neutral, or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./raw_data/Train.csv')\n",
    "test_df = pd.read_csv('./raw_data/Test.csv')\n",
    "sub = pd.read_csv('./raw_data/SampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick look at some tweets\n",
    "\n",
    "Let's have a glimpse as to what pro-vaccination, neutral and anti-vaccination tweets look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neutral\n",
    "train_df[train_df['label'] == 0]['safe_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pro-vaccination\n",
    "train_df[train_df['label'] == 1]['safe_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anti-vaccination\n",
    "train_df[train_df['label'] == -1]['safe_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing out the outlier label\n",
    "train_df = train_df[train_df['label'].isin([-1, 0, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "plt.title('Class Distributions')\n",
    "train_df.label.value_counts().plot(kind='bar', color=('green', 'gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing:\n",
    "* Remove null labelled tweet and randomly impute null tweet in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df[test_df['safe_text'].isnull() == True]\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.fillna(value='am ok with it as long as its not dangerous', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into documents/features and labels\n",
    "X = train_df.safe_text\n",
    "y = train_df.label\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = X.tolist()\n",
    "test_corpus = test_df.safe_text.tolist()\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_corpus)\n",
    "vocab_size = len(tokenizer.word_counts)\n",
    "print(f'Train Vocabulary size: {vocab_size}')\n",
    "\n",
    "# Sequence lengths (vocabulary size in a given sequence)\n",
    "# Computing the vocabulary size per percentile\n",
    "seq_lengths = np.array([len(s.split()) for s in train_corpus])\n",
    "print([(p, np.percentile(seq_lengths, p)) for p in [75, 80, 90, 95, 99, 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 33\n",
    "\n",
    "# Train encodings (words/sentences >> int) with padding\n",
    "# Padding ensures that sequences are of the same length\n",
    "train_encodings = tokenizer.texts_to_sequences(train_corpus)\n",
    "train_encodings = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    train_encodings, maxlen = max_seqlen)\n",
    "labels = np.array(y)\n",
    "\n",
    "# Creating a train dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_encodings, labels))\n",
    "\n",
    "# Test encodings with padding\n",
    "test_encodings = tokenizer.texts_to_sequences(test_corpus)\n",
    "test_encodings = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    test_encodings, maxlen= max_seqlen)\n",
    "test_labels = np.zeros(5177) # Predictions placeholder\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    " (test_encodings, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train and test batches\n",
    "\n",
    "# Train_val split and batch creation\n",
    "dataset = dataset.shuffle(1000)\n",
    "\n",
    "val_size = (len(train_corpus)) // 6\n",
    "val_dataset = dataset.take(val_size)\n",
    "train_dataset = dataset.skip(val_size)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "print(val_size)\n",
    "\n",
    "# Batching the test datset\n",
    "test_batched = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "The model is a 6-layer NN:\n",
    "     * An Embedding layer (to generate word embeddings)\n",
    "     2 stacked LSTM layers\n",
    "     2 hidden Dense layers with the `relu` activation function\n",
    "     An output Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean((K.square(y_pred - y_true))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=33\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size+1, embedding_dim),\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(max_seqlen, return_sequences=True)),\n",
    "        layers.Bidirectional(layers.LSTM(8)),\n",
    "    #layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(32, activation='relu'), #, kernel_regularizer=regularizers.l2(0.02)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[rmse])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_rmse',\n",
    "                                                               patience=3, \n",
    "                                                               verbose=1, \n",
    "                                                               factor=0.5, \n",
    "                                                               min_lr=0.00001\n",
    "                                                              )\n",
    "\n",
    "best_model_file = os.path.join('./', \"best_model.h5\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\n",
    "                                                save_weights_only=True,\n",
    "                                                save_best_only=True\n",
    "                                               )\n",
    "callbacks = [checkpoint, early_stopping, learning_rate_reduction]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_batched)\n",
    "\n",
    "# Padding\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] > 1:\n",
    "        predictions[i] = 1\n",
    "    elif predictions[i] < -1:\n",
    "        predictions[i] = -1\n",
    "        \n",
    "sub['label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min(sub.label)\n",
    "score = list(history.history.values())\n",
    "RMSE = score[-2][-1]\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('./submissions')\n",
    "sub.to_csv(f\"./submissions/sub_nn_{RMSE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
